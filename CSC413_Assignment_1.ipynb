{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSC413 Assignment 1",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd1PnGIW_xJx"
      },
      "source": [
        "# CSC413 Assignment 1: Word Embeddings\n",
        "\n",
        "**Deadline**: February 5, 2021 by 10pm\n",
        "\n",
        "**Submission**: Submit a PDF report containing your code, outputs,\n",
        "and your written solutions.\n",
        "You may export the completed notebook, but if you do so\n",
        "**it is your responsibly to make sure that your code and answers do not get cut off**.\n",
        "\n",
        "**Late Submission**: Please see the syllabus for the late submission criteria.\n",
        "\n",
        "**Working with a partner**: You may work with a partner for this assignment.\n",
        "If you decide to work with a partner, please create your group on Markus by\n",
        "February 5, 10pm, even if you intend to use grace tokens. Markus does not allow\n",
        "you to create groups past the deadline, even if you have grace tokens remaining.\n",
        "\n",
        "Based on an assignment by George Dahl, Jing Yao Li, and Roger Grosse\n",
        "\n",
        "In this assignment, we will make a neural network that can predict the next word\n",
        "in a sentence given the previous three. We will apply an idea called *weight sharing*\n",
        "to go beyond multi-layer perceptrons with only fully-connected layers.\n",
        "\n",
        "We will also solve this problem problem twice: once in numpy, and once\n",
        "using PyTorch. When using numpy, you'll implement the backpropagation\n",
        "computation manually.\n",
        "\n",
        "The prediction task is not very interesting on its own, but in learning to predict\n",
        "subsequent words given the previous three, our neural networks will learn\n",
        "about how to *represent* words. In the last part of the assignment, we'll explore\n",
        "the *vector representations* of words that our model produces, and analyze these\n",
        "representations.\n",
        "\n",
        "You may modify the starter code, including changing the signatures of helper \n",
        "functions and adding/removing helper functions. However, please make sure that your\n",
        "TA can understand what you are doing and why."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJv_7ESs_xJ0"
      },
      "source": [
        "import pandas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRx6OEsn_xJ1"
      },
      "source": [
        "## Question 1. Data\n",
        "\n",
        "With any machine learning problem, the first thing that we would want to do\n",
        "is to get an intuitive understanding of what our data looks like.\n",
        "Download the file `raw_sentences.txt` from Quercus.\n",
        "\n",
        "If you're using Google Colab, upload the file to Google Drive.\n",
        "Then, mount Google Drive from your Google Colab notebook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTc1EWiI_xJ2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drMs3puq_xJ2"
      },
      "source": [
        "Find the path to `raw_sentences.txt`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg7eNJYN_xJ3"
      },
      "source": [
        "file_path = '/content/gdrive/My Drive/CSC413/raw_sentences.txt' # TODO - UPDATE ME!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T0gACOD_xJ3"
      },
      "source": [
        "You might find it helpful to know that you can run shell commands (like `ls`) by\n",
        "using `!` in Google Colab, like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAHsIJpT_xJ3"
      },
      "source": [
        "!ls /content/gdrive/My\\ Drive/\n",
        "!mkdir /content/gdrive/My\\ Drive/CSC413"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPpYsZ-y_xJ4"
      },
      "source": [
        "The following code reads the sentences in our file, split each sentence into\n",
        "its individual words, and stores the sentences (list of words) in the\n",
        "variable `sentences`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuCOipNu_xJ4"
      },
      "source": [
        "sentences = []\n",
        "for line in open(file_path):\n",
        "    words = line.split()\n",
        "    sentence = [word.lower() for word in words]\n",
        "    sentences.append(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXs4ysms_xJ5"
      },
      "source": [
        "There are 97,162 sentences in total, and \n",
        "these sentences are composed of 250 distinct words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8lAYhBO_xJ5"
      },
      "source": [
        "vocab = set([w for s in sentences for w in s])\n",
        "print(len(sentences)) # 97162\n",
        "print(len(vocab)) # 250"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQOmtxif_xJ5"
      },
      "source": [
        "We'll separate our data into training, validation, and test.\n",
        "We'll use 10,000 sentences for test, 10,000 for validation, and\n",
        "the rest for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJEmz_jv_xJ6"
      },
      "source": [
        "test, valid, train = sentences[:10000], sentences[10000:20000], sentences[20000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq8_9K85_xJ6"
      },
      "source": [
        "### Part (a) -- 2 pts\n",
        "\n",
        "Display 10 sentences in the training set.\n",
        "Explain how punctuations are treated in our word representation, and how words\n",
        "with apostrophes are represented.\n",
        "\n",
        "(Note that for questions like this, you'll need to supply both your code **and**\n",
        "the output of your code to earn full credit.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKIU8CY0_xJ6"
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DAmLrFJ_xJ6"
      },
      "source": [
        "### Part (b) -- 2 pts\n",
        "\n",
        "What are the 10 most common words in the vocabulary? How often does each of these\n",
        "words appear in the training sentences? Express the second quantity a percentage\n",
        "(i.e. number of occurrences of the  word / total number of words in the training set).\n",
        "\n",
        "These are good quantities to compute, because one of the first things that most\n",
        "machine learning model will learn is to predict the **most common** class.\n",
        "Getting a sense of the distribution of our data will help you understand our\n",
        "model's behaviour.\n",
        "\n",
        "You might find Python's `collections.Counter` class helpful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkZqh_CC_xJ7"
      },
      "source": [
        "# Your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4fR1Ep8_xJ7"
      },
      "source": [
        "### Part (c) -- 4 pts\n",
        "\n",
        "Complete the helper functions `convert_words_to_indices` and\n",
        "`generate_4grams`, so that the function `process_data` will take a \n",
        "list of sentences (i.e. list of list of words), and generate an \n",
        "$N \\times 4$ numpy matrix containing indices of 4 words that appear\n",
        "next to each other. You can use the constants `vocab`, `vocab_itos`,\n",
        "and `vocab_stoi` in your code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2-wLQNb_xJ8"
      },
      "source": [
        "# A list of all the words in the data set. We will assign a unique \n",
        "# identifier for each of these words.\n",
        "vocab = sorted(list(set([w for s in train for w in s])))\n",
        "# A mapping of index => word (string)\n",
        "vocab_itos = dict(enumerate(vocab))\n",
        "# A mapping of word => its index\n",
        "vocab_stoi = {word:index for index, word in vocab_itos.items()}\n",
        "\n",
        "def convert_words_to_indices(sents):\n",
        "    \"\"\"\n",
        "    This function takes a list of sentences (list of list of words)\n",
        "    and returns a new list with the same structure, but where each word\n",
        "    is replaced by its index in `vocab_stoi`.\n",
        "\n",
        "    Example:\n",
        "    >>> convert_words_to_indices([['one', 'in', 'five', 'are', 'over', 'here'],\n",
        "                                  ['other', 'one', 'since', 'yesterday'],\n",
        "                                  ['you']])\n",
        "    [[148, 98, 70, 23, 154, 89], [151, 148, 181, 246], [248]]\n",
        "    \"\"\"\n",
        "\n",
        "    # Write your code here\n",
        "\n",
        "def generate_4grams(seqs):\n",
        "    \"\"\"\n",
        "    This function takes a list of sentences (list of lists) and returns\n",
        "    a new list containing the 4-grams (four consequentively occuring words)\n",
        "    that appear in the sentences. Note that a unique 4-gram can appear multiple\n",
        "    times, one per each time that the 4-gram appears in the data parameter `seqs`.\n",
        "\n",
        "    Example:\n",
        "\n",
        "    >>> generate_4grams([[148, 98, 70, 23, 154, 89], [151, 148, 181, 246], [248]])\n",
        "    [[148, 98, 70, 23], [98, 70, 23, 154], [70, 23, 154, 89], [151, 148, 181, 246]]\n",
        "    >>> generate_4grams([[1, 1, 1, 1, 1]])\n",
        "    [[1, 1, 1, 1], [1, 1, 1, 1]]\n",
        "    \"\"\"\n",
        "\n",
        "    # Write your code here\n",
        "\n",
        "def process_data(sents):\n",
        "    \"\"\"\n",
        "    This function takes a list of sentences (list of lists), and generates an\n",
        "    numpy matrix with shape [N, 4] containing indices of words in 4-grams.\n",
        "    \"\"\"\n",
        "    indices = convert_words_to_indices(sents)\n",
        "    fourgrams = generate_4grams(indices)\n",
        "    return np.array(fourgrams)\n",
        "\n",
        "train4grams = process_data(train)\n",
        "valid4grams = process_data(valid)\n",
        "test4grams = process_data(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atLah-2S_xJ9"
      },
      "source": [
        "## Question 2. MLP Math\n",
        "\n",
        "Suppose we were to use a 2-layer multilayer perceptron to solve this prediction\n",
        "problem. Our model will look like this:\n",
        "\n",
        "<img src=\"https://www.cs.toronto.edu/~lczhang/321/hw/p2_model1.png\" />\n",
        "\n",
        "\\begin{align*}\n",
        "\\bf{x} &= \\text{concatenation of the one-hot vector for words 1, 2 and 3} \\\\\n",
        "\\bf{m} &= \\bf{W^{(1)}} \\bf{x} + \\bf{b^{(1)}} \\\\\n",
        "\\bf{h} &= \\textrm{ReLU}(\\bf{m}) \\\\\n",
        "\\bf{z} &= \\bf{W^{(2)}} \\bf{h} + \\bf{b^{(2)}} \\\\\n",
        "\\bf{y} &= \\textrm{softmax}(\\bf{z}) \\\\\n",
        "\\end{align*}\n",
        "\n",
        "### Part (a) -- 2 pts\n",
        "\n",
        "What is the shape of the input vector $\\bf{x}$?\n",
        "What is the shape of the output vector $\\bf{y}$?\n",
        "Let $k$ represent the size of the hidden layer. What are the\n",
        "dimension of $W^{(1)}$ and $W^{(2)}$?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAPCBY9x_xJ-"
      },
      "source": [
        "# Your answer goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Mw3guCH_xJ-"
      },
      "source": [
        "### Part (b) -- 2 pts\n",
        "\n",
        "Draw a computation graph for $\\bf{y}$. Your graph should include\n",
        "the quantities $\\bf{W^{(1)}}$, $\\bf{W^{(2)}}$, $\\bf{b^{(1)}}$, $\\bf{b^{(2)}}$,\n",
        "$\\bf{x}$, $\\bf{m}$, $\\bf{h}$, $\\bf{z}$ and $\\bf{y}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uImuwBZM_xJ_"
      },
      "source": [
        "# Your answer goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr_LDM_h_xJ_"
      },
      "source": [
        "### Part (c) -- 3 pts\n",
        "\n",
        "Derive the gradient descent update rule for ${\\bf W}^{(2)}$.\n",
        "You should begin by deriving the update rule for $W^{(2)}_{ij}$,\n",
        "and then vectorize your answer. Assume that we will use the softmax\n",
        "activation and cross-entropy loss.\n",
        "\n",
        "Note: if you use the derivative of the softamx activation and\n",
        "the cross-entropy loss, you **must** derive them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_Tdef3j_xJ_"
      },
      "source": [
        "# Your answer goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUL5iZlV_xJ_"
      },
      "source": [
        "### Part (d) -- 4 pts\n",
        "\n",
        "What would be the update rule for $W^{(2)}_{ij}$, if we use the square loss\n",
        "$\\mathcal{L}_{SE}(\\bf{y}, \\bf{t}) = \\frac{1}{2}(\\bf{y} - \\bf{t})^2$ ?\n",
        "\n",
        "Show that we will not get good gradient signal\n",
        "to update $W^{(2)}_{ij}$ if we use this square loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSLL0sWF_xKA"
      },
      "source": [
        "# Your answer goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtKEzkqA_xKA"
      },
      "source": [
        "### Part (e) -- 4 pts\n",
        "\n",
        "In this question, we'll show a similar issue with using \n",
        "the sigmoid activation. Let's assume we have a deep neural network\n",
        "as follows:\n",
        "\n",
        "\\begin{align*}\n",
        "h_1 &= \\sigma(w_1 x + b_1) \\\\\n",
        "h_2 &= \\sigma(w_2 h_1 + b_2) \\\\\n",
        "\\dots\n",
        "\\end{align*}\n",
        "\n",
        "where, for simplicity, $x$, $w_1$, $b_1$, $h_1$, $w_2$, $b_2$, $h_2$, etc., are all scalars. Show that\n",
        "\n",
        "\\begin{align*}\n",
        "|\\frac{\\partial h_1}{\\partial x}| \\le \\frac{1}{4} |w_1|\n",
        "\\end{align*}\n",
        "\n",
        "In order to do so, you will need to first\n",
        "show that $\\sigma'(z) = \\sigma(z) (1 - \\sigma(z))$ (worth 1 point). \n",
        "Include a plot (or sketch) of the function $\\sigma'(z)$ (worth 1 point).\n",
        "\n",
        "### Part (f) -- 2 pts\n",
        "\n",
        "Continue from the previous question, show that for a deeper neural network.\n",
        "\n",
        "\\begin{align*}\n",
        "|\\frac{\\partial h_N}{\\partial x}| \\le \\frac{1}{4^N} |w_1| |w_2| \\cdots |w_N|\n",
        "\\end{align*}\n",
        "\n",
        "What would be a problem with this result?\n",
        "\n",
        "### Part (g) -- 1 pts\n",
        "\n",
        "Would we have the same issue as in part(f) we we replaced the sigmoid activation\n",
        "with ReLU activations? Why or why not?\n",
        "\n",
        "## Question 3. Weight Sharing - Math\n",
        "\n",
        "From this point onward, we will change our architecture to introduce weight sharing.\n",
        "In particular, the input $\\bf{x}$ consists of three one-hot vectors concatenated\n",
        "together. We can think of $\\bf{h}$ as a representation of those three words\n",
        "(all together). However, $\\bf{W^{(1)}}$ needs to learn about the first word\n",
        "separately from the second and third word, when some of the information could\n",
        "be shared. Consider the following architecture:\n",
        "\n",
        "<img src=\"https://www.cs.toronto.edu/~lczhang/321/hw/p2_model2.png\" />\n",
        "\n",
        "Here, we add an extra *embedding* layer to the neural network, where we compute\n",
        "the representation of **each** word before concatenating them together! We use\n",
        "the same weight $\\bf{W}^{(word)}$ for each of the three words:\n",
        "\n",
        "\\begin{align*}\n",
        "\\bf{x_a} &= \\textrm{the one-hot vector for word 1} \\\\\n",
        "\\bf{x_b} &= \\textrm{the one-hot vector for word 2} \\\\\n",
        "\\bf{x_c} &= \\textrm{the one-hot vector for word 3} \\\\\n",
        "\\bf{v_a} &= \\bf{W}^{(word)} \\bf{x_a} \\\\\n",
        "\\bf{v_b} &= \\bf{W}^{(word)} \\bf{x_b} \\\\\n",
        "\\bf{v_c} &= \\bf{W}^{(word)} \\bf{x_c} \\\\\n",
        "\\bf{v} &= \\textrm{concatenation of } \\bf{v_a}, \\bf{v_b}, \\bf{v_c} \\\\\n",
        "\\bf{m} &= \\bf{W^{(1)}} \\bf{v} + \\bf{b^{(1)}} \\\\\n",
        "\\bf{h} &= \\textrm{ReLU}(\\bf{m}) \\\\\n",
        "\\bf{z} &= \\bf{W^{(2)}} \\bf{h} + \\bf{b^{(2)}} \\\\\n",
        "\\bf{y} &= \\textrm{softmax}(\\bf{z}) \\\\\n",
        "\\end{align*}\n",
        "\n",
        "Note that there are no biases in the embedding layer.\n",
        "\n",
        "### Part (a) -- 4 pts\n",
        "\n",
        "Draw a computation graph for $\\bf{y}$. Your graph should include\n",
        "the quantities $\\bf{W}^{(word)}$, $\\bf{W^{(1)}}$, $\\bf{W^{(2)}}$,\n",
        "$\\bf{b^{(1)}}$, $\\bf{b^{(2)}}$,\n",
        "$\\bf{x_a}$,$\\bf{x_b}$, $\\bf{x_c}$,\n",
        "$\\bf{v_a}$,$\\bf{v_b}$, $\\bf{v_c}$, $\\bf{v}$,\n",
        "$\\bf{m}$, $\\bf{h}$, $\\bf{z}$ and $\\bf{y}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIVodhAl_xKB"
      },
      "source": [
        "# Your answer goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npGJGRBk_xKC"
      },
      "source": [
        "### Part (b) -- 2 pts\n",
        "\n",
        "Using the computation graph from part (e), use the chain rule to\n",
        "write the quantity $\\frac{\\partial{\\bf y}}{\\partial{\\bf W}^{(word)}}$\n",
        "in terms of derivatives along the edges in the computation graph\n",
        "(e.g. $\\frac{\\partial \\bf{y}}{\\partial {\\bf z}} \\frac{\\partial \\bf{z}}{\\partial {\\bf \\cdot}} \\dots$)\n",
        "\n",
        "You don't need to compute the actual derivatives along the edges\n",
        "for this question.  However, you will need to in Q4(a)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoaHHxN7_xKC"
      },
      "source": [
        "# Your answer goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHcMTE1S_xKC"
      },
      "source": [
        "## Question 4. NumPy\n",
        "\n",
        "In this question, we will implement the model from Question 3\n",
        "using NumPy.  Start by reviewing these helper functions,\n",
        "which are given to you:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kumYK-7_xKD"
      },
      "source": [
        "def make_onehot(indicies, total=250):\n",
        "    \"\"\"\n",
        "    Convert indicies into one-hot vectors by\n",
        "        1. Creating an identity matrix of shape [total, total]\n",
        "        2. Indexing the appropriate columns of that identity matrix\n",
        "    \"\"\"\n",
        "    I = np.eye(total)\n",
        "    return I[indicies]\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Compute the softmax of vector x, or row-wise for a matrix x.\n",
        "    We subtract x.max(axis=0) from each row for numerical stability.\n",
        "    \"\"\"\n",
        "    x = x.T\n",
        "    exps = np.exp(x - x.max(axis=0))\n",
        "    probs = exps / np.sum(exps, axis=0)\n",
        "    return probs.T\n",
        "\n",
        "def get_batch(data, range_min, range_max, onehot=True):\n",
        "    \"\"\"\n",
        "    Convert one batch of data in the form of 4-grams into input and output\n",
        "    data and return the training data (xs, ts) where:\n",
        "     - `xs` is an numpy array of one-hot vectors of shape [batch_size, 3, 250]\n",
        "     - `ts` is either\n",
        "            - a numpy array of shape [batch_size, 250] if onehot is True,\n",
        "            - a numpy array of shape [batch_size] containing indicies otherwise\n",
        "\n",
        "    Preconditions:\n",
        "     - `data` is a numpy array of shape [N, 4] produced by a call\n",
        "        to `process_data`\n",
        "     - range_max > range_min\n",
        "    \"\"\"\n",
        "    xs = data[range_min:range_max, :3]\n",
        "    xs = make_onehot(xs)\n",
        "    ts = data[range_min:range_max, 3]\n",
        "    if onehot:\n",
        "        ts = make_onehot(ts).reshape(-1, 250)\n",
        "    return xs, ts\n",
        "\n",
        "def estimate_accuracy(model, data, batch_size=5000, max_N=100000):\n",
        "    \"\"\"\n",
        "    Estimate the accuracy of the model on the data. To reduce\n",
        "    computation time, use at most `max_N` elements of `data` to\n",
        "    produce the estimate.\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    N = 0\n",
        "    for i in range(0, data.shape[0], batch_size):\n",
        "        xs, ts = get_batch(data, i, i + batch_size, onehot=False)\n",
        "        z = model(xs)\n",
        "        pred = np.argmax(z, axis=1)\n",
        "        correct += np.sum(ts == pred)\n",
        "        N += ts.shape[0]\n",
        "\n",
        "        if N > max_N:\n",
        "            break\n",
        "    return correct / N"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh_pV-Kr_xKD"
      },
      "source": [
        "### Part (a) -- 8 point\n",
        "\n",
        "Your first task is to implement the model from Question 3 in NumPy.\n",
        "We will represent the model as a Python class. We set up the \n",
        "class methods and APIs to be similar to that of PyTorch, so that you\n",
        "have some intuition about what PyTorch is doing under the hood.\n",
        "Here's what you need to do:\n",
        "\n",
        "1. in the `__init__` method, initialize the weights and biases to have the correct shapes. You may want to look back at your answers in the previous question. (0 points)\n",
        "2. complete the `forward` method to compute the predictions given a **batch** of inputs. This function will also store the intermediate values obtained in the computation; we will need these values for gradient descent. (3 points)\n",
        "3. complete the `backward` method to compute the gradients of the loss with respect to the weights and biases. (4 points)\n",
        "4. complete the `update` method that uses the stored gradients to update the weights and biases. (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPtbrfJp_xKD"
      },
      "source": [
        "class NumpyWordEmbModel(object):\n",
        "    def __init__(self, vocab_size=250, emb_size=100, num_hidden=100):\n",
        "        \"\"\"\n",
        "        Initialize the weights and biases to zero. Update this method\n",
        "        so that weights and baises have the correct shape.\n",
        "        \"\"\"\n",
        "        TODO = 0\n",
        "        self.vocab_size = vocab_size\n",
        "        self.emb_size = emb_size\n",
        "        self.num_hidden = num_hidden\n",
        "        self.emb_weights = np.zeros([TODO, TODO]) # W^{(word)}\n",
        "        self.weights1 = np.zeros([TODO, TODO])    # W^{(1)}\n",
        "        self.bias1 = np.zeros([TODO])             # b^{(1)}\n",
        "        self.weights2 = np.zeros([TODO, TODO])    # W^{(2)}\n",
        "        self.bias2 = np.zeros([TODO])             # b^{(2)}\n",
        "        self.cleanup()\n",
        "\n",
        "    def initializeParams(self):\n",
        "        \"\"\"\n",
        "        Randomly initialize the weights and biases of this two-layer MLP.\n",
        "        The randomization is necessary so that each weight is updated to\n",
        "        a different value.\n",
        "\n",
        "        You do not need to change this method.\n",
        "        \"\"\"\n",
        "        self.emb_weights = np.random.normal(0, 2/self.emb_size, self.emb_weights.shape)\n",
        "        self.weights1 = np.random.normal(0, 2/self.emb_size, self.weights1.shape)\n",
        "        self.bias1 = np.random.normal(0, 2/self.emb_size, self.bias1.shape)\n",
        "        self.weights2 = np.random.normal(0, 2/self.num_hidden, self.weights2.shape)\n",
        "        self.bias2 = np.random.normal(0, 2/self.num_hidden, self.bias2.shape)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Compute the forward pass prediction for inputs.\n",
        "\n",
        "        Note that for vectorization, `inputs` will be a rank-3 numpy array\n",
        "        with shape [N, 3, vocab_size], where N is the batch size.\n",
        "        The returned value will contain the predictions for the N\n",
        "        data points in the batch, so the return value shape should be\n",
        "        [N, something].\n",
        "\n",
        "        You should refer to the mathematical expressions we provided in Q3\n",
        "        when completing this method. However, because we are computing\n",
        "        forward pass for a batch of data at a time, you may need to rearrange\n",
        "        some computation (e.g. some matrix-vector multiplication will become\n",
        "        matrix-matrix multiplications, and you'll need to be careful about\n",
        "        arranging the dimensions of your matrices.)\n",
        "\n",
        "        For numerical stability reasons, we will return the **logit z**\n",
        "        instead of the **probability y**. The loss function assumes that \n",
        "        we return the logits from this function.\n",
        "\n",
        "        After writing this function, you might want to check that your code\n",
        "        runs before continuing, e.g. try\n",
        "\n",
        "            xs, ts = get_batch(train4grams, 0, 8, onehot=True)\n",
        "            m = NumpyWordEmbModel()\n",
        "            m.forward(xs)\n",
        "        \"\"\"\n",
        "        self.N = inputs.shape[0]\n",
        "        self.xa = None # todo\n",
        "        self.xb = None # todo\n",
        "        self.xc = None # todo\n",
        "        self.va = None # todo\n",
        "        self.vb = None # todo\n",
        "        self.vc = None # todo\n",
        "        self.v = None # todo\n",
        "        self.m = None # todo\n",
        "        self.h = None # todo\n",
        "        self.z = None # todo\n",
        "        self.y = softmax(self.z)\n",
        "        return self.z\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        \"\"\"\n",
        "        This function is here so that if you call the object like a function,\n",
        "        the `backward` method will get called. For example, if we have\n",
        "            m = NumpyWordEmbModel()\n",
        "        Calling `m(foo)` is equivalent to calling `m.forward(foo)`.\n",
        "\n",
        "        You do not need to change this method.\n",
        "        \"\"\"\n",
        "        return self.forward(inputs)\n",
        "\n",
        "    def backward(self, ts):\n",
        "        \"\"\"\n",
        "        Compute the backward pass, given the ground-truth, one-hot targets.\n",
        "        Note that `ts` needs to be a numpy array with shape [N, vocab_size].\n",
        "        Complete this method. You might want to refer to your answers to Q2\n",
        "        and Q3. But be careful: we are computing the backward pass for an\n",
        "        entire batch of data at a time! Carefully track the dimensions of your\n",
        "        quantities!\n",
        "\n",
        "        You may assume that the forward() method has already been called, so\n",
        "        you can access values like self.N, self.y, etc..\n",
        "\n",
        "        This function needs to be called before calling the update() method.\n",
        "        \"\"\"\n",
        "        z_bar = (self.y - ts) / self.N\n",
        "        self.w2_bar = None # todo, compute gradient for W^{(2)}\n",
        "        self.b2_bar = None # todo, compute gradient for b^{(2)}\n",
        "        h_bar = None # todo\n",
        "        m_bar = None # todo\n",
        "        self.w1_bar = None # todo\n",
        "        self.b1_bar = None # todo\n",
        "        # ...\n",
        "        self.emb_bar = None # todo, compute gradient for W^{(word)}\n",
        "\n",
        "    def update(self, alpha):\n",
        "        \"\"\"\n",
        "        Compute the gradient descent update for the parameters.\n",
        "        Complete this method. Use `alpha` as the learning rate.\n",
        "\n",
        "        You can assume that the forward() and backward() methods have already\n",
        "        been called, so you can access values like self.w1_bar.\n",
        "        \"\"\"\n",
        "        self.weights1 = self.weights1 - alpha * self.w1_bar\n",
        "        # todo... update the other weights/biases\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"\n",
        "        Erase the values of the variables that we use in our computation.\n",
        "       \n",
        "        You do not need to change this method.\n",
        "        \"\"\"\n",
        "        self.N = None\n",
        "        self.xa = None\n",
        "        self.xb = None\n",
        "        self.xc = None\n",
        "        self.va = None\n",
        "        self.vb = None\n",
        "        self.vc = None\n",
        "        self.v = None\n",
        "        self.m = None\n",
        "        self.h = None\n",
        "        self.z = None\n",
        "        self.y = None\n",
        "        self.z_bar = None\n",
        "        self.w2_bar = None\n",
        "        self.b2_bar = None\n",
        "        self.w1_bar = None\n",
        "        self.b1_bar = None\n",
        "        self.emb_bar = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jn1u29DF_xKE"
      },
      "source": [
        "### Part (b) -- 2 points\n",
        "\n",
        "Complete the `run_gradient_descent` function. Train your numpy model\n",
        "to obtain a training accuracy of at least 25%. You do not need to train\n",
        "this model to convergence, but you do need to clearly show\n",
        "that your model reached at least 25% training accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "281yRC4i_xKE"
      },
      "source": [
        "def run_gradient_descent(model,\n",
        "                         train_data=train4grams,\n",
        "                         validation_data=valid4grams,\n",
        "                         batch_size=100,\n",
        "                         learning_rate=0.1,\n",
        "                         max_iters=5000):\n",
        "    \"\"\"\n",
        "    Use gradient descent to train the numpy model on the dataset train4grams.\n",
        "    \"\"\"\n",
        "    n = 0\n",
        "    while n < max_iters:\n",
        "        # shuffle the training data, and break early if we don't have\n",
        "        # enough data to remaining in the batch\n",
        "        np.random.shuffle(train_data)\n",
        "        for i in range(0, train_data.shape[0], batch_size):\n",
        "            if (i + batch_size) > train_data.shape[0]:\n",
        "                break\n",
        "\n",
        "            # get the input and targets of a minibatch\n",
        "            xs, ts = get_batch(train_data, i, i + batch_size, onehot=True)\n",
        "\n",
        "            # erase any accumulated gradients\n",
        "            model.cleanup()\n",
        "\n",
        "            # forward pass: compute prediction\n",
        "\n",
        "            # TODO: add your code here\n",
        "\n",
        "            # backward pass: compute error \n",
        "            \n",
        "\n",
        "            # increment the iteration count\n",
        "            n += 1\n",
        "\n",
        "            # compute and plot the *validation* loss and accuracy\n",
        "            if (n % 100 == 0):\n",
        "                train_cost = -np.sum(ts * np.log(y)) / batch_size\n",
        "                train_acc = estimate_accuracy(model, train_data)\n",
        "                val_acc = estimate_accuracy(model, validation_data)\n",
        "                model.cleanup()\n",
        "                print(\"Iter %d. [Val Acc %.0f%%] [Train Acc %.0f%%, Loss %f]\" % (\n",
        "                      n, val_acc * 100, train_acc * 100, train_cost))\n",
        "\n",
        "        if n >= max_iters:\n",
        "            return\n",
        "\n",
        "\n",
        "numpy_model= NumpyWordEmbModel()\n",
        "numpy_model.initializeParams()\n",
        "# run_gradient_descent(...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ6ei464_xKE"
      },
      "source": [
        "### Part (c) -- 2 pts\n",
        "\n",
        "If we do not call `numpy_model.initializeParams()`, your model weights will\n",
        "not change. Clearly explain (mathematically) why this is the case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arbuFdRb_xKF"
      },
      "source": [
        "# Your answer goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAVphSvp_xKF"
      },
      "source": [
        "### Part (d) -- 2 pts\n",
        "\n",
        "The `estimate_accuracy` function takes the continuous predictions `z`\n",
        "and turns it into a discrete prediction `pred`. Show that for a given\n",
        "data point, `pred` is equal to 1 only if the predictive probability `y`\n",
        "is at least 0.5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "535O3SG6_xKF"
      },
      "source": [
        "# Your answer goes here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8fTukMG_xKF"
      },
      "source": [
        "## Question 5. PyTorch\n",
        "\n",
        "Now, we will build the same model in PyTorch.\n",
        "\n",
        "### Part (a) -- 2 pts\n",
        "\n",
        "Since PyTorch uses\n",
        "automatic differentiation, we only need to write the *forward pass* of our\n",
        "model. Complete the `__init__` and `forward` methods below.\n",
        "\n",
        "Hint: You might want to look up the `reshape` method in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Chu2H-NV_xKF"
      },
      "source": [
        "class PyTorchWordEmb(nn.Module):\n",
        "    def __init__(self, emb_size=100, num_hidden=300, vocab_size=250):\n",
        "        super(PyTorchWordEmb, self).__init__()\n",
        "        TODO = 0\n",
        "        self.word_emb_layer = nn.Linear(TODO,      # num input W^(word)\n",
        "                                        TODO,      # num output W^(word)\n",
        "                                        bias=False)\n",
        "        self.fc_layer1 = nn.Linear(TODO, # num input W^(1)\n",
        "                                   TODO) # num output W^(1)\n",
        "        self.fc_layer2 = nn.Linear(TODO, # num input W^(2)\n",
        "                                   TODO) # num output W^(2)\n",
        "        self.num_hidden = num_hidden\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, inp):\n",
        "        vs = self.word_emb_layer(inp)\n",
        "        v = None # TODO: what do you need to do here?\n",
        "        m = self.fc_layer1(v)\n",
        "        h = torch.relu(m)\n",
        "        z = None # TODO: what do you need to do here?\n",
        "        return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23__-psW_xKG"
      },
      "source": [
        "### Part (b) -- 2 pts\n",
        "\n",
        "The function `run_pytorch_gradient_descent` is given to you. It is similar\n",
        "to the code that you wrote fro the PyTorch model, with a few differences:\n",
        "\n",
        "1. We will use a slightly fancier optimizer called **Adam**. For this optimizer,\n",
        "   a smaller learning rate usually works better, so the default learning\n",
        "   rate is set to 0.001.\n",
        "2. Since we get weight decay for free, you are welcome to use weight decay.\n",
        "\n",
        "\n",
        "Use this function and train your PyTorch model to obtain a training\n",
        "accuracy of at least 37%.  Plot the learning curve using the `plot_learning_curve`\n",
        "function provided to you, and include your plot in your PDF submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNmhnqmG_xKG"
      },
      "source": [
        "def estimate_accuracy_torch(model, data, batch_size=5000, max_N=100000):\n",
        "    \"\"\"\n",
        "    Estimate the accuracy of the model on the data. To reduce\n",
        "    computation time, use at most `max_N` elements of `data` to\n",
        "    produce the estimate.\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    N = 0\n",
        "    for i in range(0, data.shape[0], batch_size):\n",
        "        # get a batch of data\n",
        "        xs, ts = get_batch(data, i, i + batch_size, onehot=False)\n",
        "        \n",
        "        # forward pass prediction\n",
        "        z = model(torch.Tensor(xs))\n",
        "        z = z.detach().numpy() # convert the PyTorch tensor => numpy array\n",
        "        pred = np.argmax(z, axis=1)\n",
        "        correct += np.sum(pred == ts)\n",
        "        N += ts.shape[0]\n",
        "\n",
        "        if N > max_N:\n",
        "            break\n",
        "    return correct / N\n",
        "\n",
        "def run_pytorch_gradient_descent(model,\n",
        "                                 train_data=train4grams,\n",
        "                                 validation_data=valid4grams,\n",
        "                                 batch_size=100,\n",
        "                                 learning_rate=0.001,\n",
        "                                 weight_decay=0,\n",
        "                                 max_iters=1000,\n",
        "                                 checkpoint_path=None):\n",
        "    \"\"\"\n",
        "    Train the PyTorch model on the dataset `train_data`, reporting\n",
        "    the validation accuracy on `validation_data`, for `max_iters`\n",
        "    iteration.\n",
        "\n",
        "    If you want to **checkpoint** your model weights (i.e. save the\n",
        "    model weights to Google Drive), then the parameter\n",
        "    `checkpoint_path` should be a string path with `{}` to be replaced\n",
        "    by the iteration count:\n",
        "\n",
        "    For example, calling \n",
        "\n",
        "    >>> run_pytorch_gradient_descent(model, ...,\n",
        "            checkpoint_path = '/content/gdrive/My Drive/CSC413/mlp/ckpt-{}.pk')\n",
        "\n",
        "    will save the model parameters in Google Drive every 500 iterations.\n",
        "    You will have to make sure that the path exists (i.e. you'll need to create\n",
        "    the folder CSC413, mlp, etc...). Your Google Drive will be populated with files:\n",
        "\n",
        "    - /content/gdrive/My Drive/CSC413/mlp/ckpt-500.pk\n",
        "    - /content/gdrive/My Drive/CSC413/mlp/ckpt-1000.pk\n",
        "    - ...\n",
        "\n",
        "    To load the weights at a later time, you can run:\n",
        "\n",
        "    >>> model.load_state_dict(torch.load('/content/gdrive/My Drive/CSC413/mlp/ckpt-500.pk'))\n",
        "\n",
        "    This function returns the training loss, and the training/validation accuracy,\n",
        "    which we can use to plot the learning curve.\n",
        "    \"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr=learning_rate,\n",
        "                           weight_decay=weight_decay)\n",
        "\n",
        "    iters, losses = [], []\n",
        "    iters_sub, train_accs, val_accs  = [], [] ,[]\n",
        "\n",
        "    n = 0 # the number of iterations\n",
        "    while True:\n",
        "        for i in range(0, train_data.shape[0], batch_size):\n",
        "            if (i + batch_size) > train_data.shape[0]:\n",
        "                break\n",
        "\n",
        "            # get the input and targets of a minibatch\n",
        "            xs, ts = get_batch(train_data, i, i + batch_size, onehot=False)\n",
        "\n",
        "            # convert from numpy arrays to PyTorch tensors\n",
        "            xs = torch.Tensor(xs)\n",
        "            ts = torch.Tensor(ts).long()\n",
        "\n",
        "            zs = model(xs)\n",
        "            loss = criterion(zs, ts) # compute the total loss\n",
        "            loss.backward()          # compute updates for each parameter\n",
        "            optimizer.step()         # make the updates for each parameter\n",
        "            optimizer.zero_grad()    # a clean up step for PyTorch\n",
        "\n",
        "            # save the current training information\n",
        "            iters.append(n)\n",
        "            losses.append(float(loss)/batch_size)  # compute *average* loss\n",
        "\n",
        "            if n % 500 == 0:\n",
        "                iters_sub.append(n)\n",
        "                train_cost = float(loss.detach().numpy())\n",
        "                train_acc = estimate_accuracy_torch(model, train_data)\n",
        "                train_accs.append(train_acc)\n",
        "                val_acc = estimate_accuracy_torch(model, validation_data)\n",
        "                val_accs.append(val_acc)\n",
        "                print(\"Iter %d. [Val Acc %.0f%%] [Train Acc %.0f%%, Loss %f]\" % (\n",
        "                      n, val_acc * 100, train_acc * 100, train_cost))\n",
        "\n",
        "                if (checkpoint_path is not None) and n > 0:\n",
        "                    torch.save(model.state_dict(), checkpoint_path.format(n))\n",
        "\n",
        "            # increment the iteration number\n",
        "            n += 1\n",
        "\n",
        "            if n > max_iters:\n",
        "                return iters, losses, iters_sub, train_accs, val_accs\n",
        "\n",
        "\n",
        "def plot_learning_curve(iters, losses, iters_sub, train_accs, val_accs):\n",
        "    \"\"\"\n",
        "    Plot the learning curve.\n",
        "    \"\"\"\n",
        "    plt.title(\"Learning Curve: Loss per Iteration\")\n",
        "    plt.plot(iters, losses, label=\"Train\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Learning Curve: Accuracy per Iteration\")\n",
        "    plt.plot(iters_sub, train_accs, label=\"Train\")\n",
        "    plt.plot(iters_sub, val_accs, label=\"Validation\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECdovJmB_xKG"
      },
      "source": [
        "pytorch_model = PyTorchWordEmb()\n",
        "# learning_curve_info = run_pytorch_gradient_descent(pytorch_model, ...)\n",
        "\n",
        "# you might want to save the `learning_curve_info` somewhere, so that you can plot\n",
        "# the learning curve prior to exporting your PDF file\n",
        "\n",
        "# plot_learning_curve(*learning_curve_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0FiLeLy_xKG"
      },
      "source": [
        "### Part (c) -- 3 points\n",
        "\n",
        "Write a function `make_prediction` that takes as parameters\n",
        "a PyTorchWordEmb model and sentence (a list of words), and produces\n",
        "a prediction for the next word in the sentence.\n",
        "\n",
        "Start by thinking about what you need to do, step by step, taking\n",
        "care of the difference between a numpy array and a PyTorch Tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZOWzVrf_xKG"
      },
      "source": [
        "def make_prediction_torch(model, sentence):\n",
        "    \"\"\"\n",
        "    Use the model to make a prediction for the next word in the\n",
        "    sentence using the last 3 words (sentence[:-3]). You may assume\n",
        "    that len(sentence) >= 3 and that `model` is an instance of\n",
        "    PyTorchWordEmb. You might find the function torch.argmax helpful.\n",
        "\n",
        "    This function should return the next word, represented as a string.\n",
        "\n",
        "    Example call:\n",
        "    >>> make_prediction_torch(pytorch_model, ['you', 'are', 'a'])\n",
        "    \"\"\"\n",
        "    global vocab_stoi, vocab_itos\n",
        "\n",
        "    #  Write your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV3PCj2G_xKG"
      },
      "source": [
        "### Part (d) -- 4 points\n",
        "\n",
        "Use your code to predict what the next word should be in each\n",
        "of the following sentences:\n",
        "\n",
        "- \"You are a\"\n",
        "- \"few companies show\"\n",
        "- \"There are no\"\n",
        "- \"yesterday i was\"\n",
        "- \"the game had\"\n",
        "- \"yesterday the federal\"\n",
        "\n",
        "Do your predictions make sense? (If all of your predictions are the same,\n",
        "train your model for more iterations, or change the hyper parameters in your\n",
        "model. You may need to do this even if your training accuracy is >=37%)\n",
        "\n",
        "One concern you might have is that our model may be \"memorizing\" information\n",
        "from the training set.  Check if each of 3-grams (the 3 words appearing next\n",
        "to each other) appear in the training set. If so, what word occurs immediately\n",
        "following those three words?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6eHUPcc_xKH"
      },
      "source": [
        "# Write your code and answers here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZRgJPON_xKH"
      },
      "source": [
        "### Part (3) -- 1 points\n",
        "\n",
        "Report the test accuracy of your model. The test accuracy is the percentage\n",
        "of correct predictions across your test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ddUHOMV_xKH"
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDN4DsJx_xKH"
      },
      "source": [
        "## Question 6. Visualizing Word Embeddings\n",
        "\n",
        "While training the `PyTorchWordEmb`, we trained the `word_emb_layer`, which takes a one-hot\n",
        "representation of a word in our vocabulary, and returns a low-dimensional vector\n",
        "representation of that word. In this question, we will explore these word embeddings.\n",
        "\n",
        "### Part (a) -- 1 pts\n",
        "\n",
        "The code below extracts the **weights** of the word embedding layer,\n",
        "and converts the PyTorch tensor into an numpy array.\n",
        "Explain why each *row* of `word_emb` contains the vector representing\n",
        "of a word. For example `word_emb[vocab_stoi[\"any\"],:]` contains the\n",
        "vector representation of the word \"any\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd08r0eH_xKH"
      },
      "source": [
        "word_emb_weights = list(pytorch_model.word_emb_layer.parameters())[0]\n",
        "word_emb = word_emb_weights.detach().numpy().T\n",
        "\n",
        "# Write your explanation here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCMGwkBL_xKI"
      },
      "source": [
        "### Part (b) -- 1 pts\n",
        "\n",
        "Once interesting thing about these word embeddings is that distances\n",
        "in these vector representations of words make some sense! To show this,\n",
        "we have provided code below that computes the cosine similarity of\n",
        "every pair of words in our vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3rOce6e_xKI"
      },
      "source": [
        "norms = np.linalg.norm(word_emb, axis=1)\n",
        "word_emb_norm = (word_emb.T / norms).T\n",
        "similarities = np.matmul(word_emb_norm, word_emb_norm.T)\n",
        "\n",
        "# Some example distances. The first one should be larger than the second\n",
        "print(similarities[vocab_stoi['any'], vocab_stoi['many']])\n",
        "print(similarities[vocab_stoi['any'], vocab_stoi['government']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8_n-5Iq_xKI"
      },
      "source": [
        "Compute the 5 closest words to the following words:\n",
        "\n",
        "- \"four\"\n",
        "- \"go\"\n",
        "- \"what\"\n",
        "- \"should\"\n",
        "- \"school\"\n",
        "- \"your\"\n",
        "- \"yesterday\"\n",
        "- \"not\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U5SNoFR_xKI"
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpD2yhsq_xKI"
      },
      "source": [
        "### Part (c) -- 2 pts\n",
        "\n",
        "We can visualize the word embeddings by reducing the dimensionality of\n",
        "the word vectors to 2D. There are many dimensionality reduction techniques\n",
        "that we could use, and we will use an algorithm called t-SNE.\n",
        "(You don’t need to know what this is for the assignment,\n",
        "but we may cover it later in the course.)\n",
        "Nearby points in this 2-D space are meant to correspond to nearby points\n",
        "in the original, high-dimensional space.\n",
        "\n",
        "The following code runs the t-SNE algorithm and plots the result.\n",
        "Look at the plot and find two clusters of related words.\n",
        "What do the words in each cluster have in common?\n",
        "\n",
        "Note that there is randomness in the initialization of the t-SNE \n",
        "algorithm. If you re-run this code, you may get a different image.\n",
        "Please make sure to submit your image in the PDF file for your TA to see."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ_qMtaU_xKJ"
      },
      "source": [
        "import sklearn.manifold\n",
        "tsne = sklearn.manifold.TSNE()\n",
        "Y = tsne.fit_transform(word_emb)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.xlim(Y[:,0].min(), Y[:, 0].max())\n",
        "plt.ylim(Y[:,1].min(), Y[:, 1].max())\n",
        "for i, w in enumerate(vocab):\n",
        "    plt.text(Y[i, 0], Y[i, 1], w)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFoAHgx4_xKJ"
      },
      "source": [
        "## Question 7. Work Allocation -- 2 pts\n",
        "\n",
        "This question is to make sure that if you are working with a partner, that\n",
        "you and your partner contributed equally to the assignment.\n",
        "\n",
        "Please have each team member write down the times that you worked on the\n",
        "assignment, and your contribution to the assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljkKD0Xk_xKJ"
      },
      "source": [
        "# Example answer:\n",
        "# I worked on the assignment on Jan 20 afternoon, Jan 26th 12pm-2pm,\n",
        "# and then Feb 4th in the evening. My partner and I had a meeting on\n",
        "# Jan 20th to read the entire assignment, and we did Question 1 together\n",
        "# while screensharing. I worked out the math for Q2, and checked my\n",
        "# partner's implementation in Q3. I also wrote the Q3 helper functions,\n",
        "# and Q4(b)."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}